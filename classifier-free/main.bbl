\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chen et~al.(2021)Chen, Zhang, Zen, Weiss, Norouzi, and
  Chan]{chen2020wavegrad}
Nanxin Chen, Yu~Zhang, Heiga Zen, Ron~J Weiss, Mohammad Norouzi, and William
  Chan.
\newblock {WaveGrad}: Estimating gradients for waveform generation.
\newblock \emph{{International Conference on Learning Representations}}, 2021.

\bibitem[Dhariwal \& Nichol(2021)Dhariwal and Nichol]{dhariwal2021diffusion}
Prafulla Dhariwal and Alex Nichol.
\newblock Diffusion models beat {GAN}s on image synthesis.
\newblock \emph{arXiv preprint arXiv:2105.05233}, 2021.

\bibitem[Grandvalet \& Bengio(2004)Grandvalet and Bengio]{grandvalet2004semi}
Yves Grandvalet and Yoshua Bengio.
\newblock Semi-supervised learning by entropy minimization.
\newblock In \emph{Proceedings of the 17th International Conference on Neural
  Information Processing Systems}, pp.\  529--536, 2004.

\bibitem[Gr{\"u}nwald \& Langford(2007)Gr{\"u}nwald and
  Langford]{grunwald2007suboptimal}
Peter Gr{\"u}nwald and John Langford.
\newblock Suboptimal behavior of bayes and mdl in classification under
  misspecification.
\newblock \emph{Machine Learning}, 66\penalty0 (2-3):\penalty0 119--149, 2007.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock {GANs} trained by a two time-scale update rule converge to a local
  {Nash} equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6626--6637, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6840--6851, 2020.

\bibitem[Ho et~al.(2021)Ho, Saharia, Chan, Fleet, Norouzi, and
  Salimans]{ho2021cascaded}
Jonathan Ho, Chitwan Saharia, William Chan, David~J Fleet, Mohammad Norouzi,
  and Tim Salimans.
\newblock Cascaded diffusion models for high fidelity image generation.
\newblock \emph{arXiv preprint arXiv:2106.15282}, 2021.

\bibitem[Hyv{\"a}rinen \& Dayan(2005)Hyv{\"a}rinen and
  Dayan]{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen and Peter Dayan.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (4), 2005.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Diederik~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10215--10224, 2018.

\bibitem[Kingma et~al.(2021)Kingma, Salimans, Poole, and
  Ho]{kingma2021variational}
Diederik~P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho.
\newblock Variational diffusion models.
\newblock \emph{arXiv preprint arXiv:2107.00630}, 2021.

\bibitem[Kong et~al.(2021)Kong, Ping, Huang, Zhao, and
  Catanzaro]{kong2020diffwave}
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.
\newblock {DiffWave: A Versatile Diffusion Model for Audio Synthesis}.
\newblock \emph{{International Conference on Learning Representations}}, 2021.

\bibitem[Nichol \& Dhariwal(2021)Nichol and Dhariwal]{nichol2021improved}
Alex Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock \emph{{International Conference on Machine Learning}}, 2021.

\bibitem[Razavi et~al.(2019)Razavi, van~den Oord, and
  Vinyals]{razavi2019generating}
Ali Razavi, Aaron van~den Oord, and Oriol Vinyals.
\newblock Generating diverse high-fidelity images with {VQ-VAE-2}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  14837--14847, 2019.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock {ImageNet} large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Salimans \& Ho(2021)Salimans and Ho]{salimans2021should}
Tim Salimans and Jonathan Ho.
\newblock Should {EBM}s model the energy or the score?
\newblock In \emph{Energy Based Models Workshop-ICLR 2021}, 2021.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training {GAN}s.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2234--2242, 2016.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2256--2265, 2015.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11895--11907, 2019.

\bibitem[Song et~al.(2021{\natexlab{a}})Song, Durkan, Murray, and
  Ermon]{song2021maximum}
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
\newblock Maximum likelihood training of score-based diffusion models.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2101, 2021{\natexlab{a}}.

\bibitem[Song et~al.(2021{\natexlab{b}})Song, Sohl-Dickstein, Kingma, Kumar,
  Ermon, and Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock \emph{{International Conference on Learning Representations}},
  2021{\natexlab{b}}.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural Computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Wu et~al.(2019)Wu, Donahue, Balduzzi, Simonyan, and
  Lillicrap]{wu2019logan}
Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap.
\newblock {LOGAN}: Latent optimisation for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1912.00953}, 2019.

\end{thebibliography}
